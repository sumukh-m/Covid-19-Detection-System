{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom Model CT Scan",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1HQrtNf4jmH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "import sklearn\n",
        "from glob import glob\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = [224, 224] # feel free to change depending on dataset\n",
        "\n",
        "# training config:\n",
        "epochs = 500\n",
        "batch_size = 32\n",
        "\n",
        "#define paths\n",
        "covid_path = 'data/ct/CT_COVID'\n",
        "noncovid_path = 'data/ct/CT_NonCOVID'\n",
        "\n",
        "# Use glob to grab images from path .jpg or jpeg\n",
        "covid_files = glob(covid_path + '/*')\n",
        "noncovid_files = glob(noncovid_path + '/*')"
      ],
      "metadata": {
        "id": "7Db83SJW4q6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing Labels\n",
        "covid_labels = []\n",
        "noncovid_labels = []\n",
        "\n",
        "covid_images=[]\n",
        "noncovid_images=[]\n",
        "\n",
        "import cv2 \n",
        "\n",
        "for i in range(len(covid_files)):\n",
        "  image = cv2.imread(covid_files[i])\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  image = cv2.resize(image,(224,224))\n",
        "  image = np.transpose(image, (2, 1, 0))\n",
        "  covid_images.append(image)\n",
        "  covid_labels.append('Chest_COVID')\n",
        "\n",
        "for i in range(len(noncovid_files)):\n",
        "  image = cv2.imread(noncovid_files[i])\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  image = cv2.resize(image,(224,224))\n",
        "  image = np.transpose(image, (2, 1, 0))\n",
        "  noncovid_images.append(image)\n",
        "  noncovid_labels.append('Chest_NonCOVID')"
      ],
      "metadata": {
        "id": "I66oA__t5Sry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize to interval of [0,1]\n",
        "covid_images = np.array(covid_images) / 255\n",
        "noncovid_images = np.array(noncovid_images) / 255"
      ],
      "metadata": {
        "id": "VhEeRSXa5Utr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# split into training and testing\n",
        "covid_x_train, covid_x_test, covid_y_train, covid_y_test = train_test_split(\n",
        "    covid_images, covid_labels, test_size=0.2)\n",
        "noncovid_x_train, noncovid_x_test, noncovid_y_train, noncovid_y_test = train_test_split(\n",
        "    noncovid_images, noncovid_labels, test_size=0.2)\n",
        "\n",
        "\n",
        "X_train = np.concatenate((noncovid_x_train, covid_x_train), axis=0)\n",
        "X_test = np.concatenate((noncovid_x_test, covid_x_test), axis=0)\n",
        "y_train = np.concatenate((noncovid_y_train, covid_y_train), axis=0)\n",
        "y_test = np.concatenate((noncovid_y_test, covid_y_test), axis=0)\n",
        "\n",
        "# make labels into categories - either 0 or 1\n",
        "y_train = LabelBinarizer().fit_transform(y_train)\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "y_test = LabelBinarizer().fit_transform(y_test)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "nwOxxRLy5WMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(25088, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "net = Net()\n",
        "net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "a4lEWMQ85X52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xt_train=torch.from_numpy(X_train)\n",
        "yt_train = torch.from_numpy(y_train)\n",
        "\n",
        "Xt_test = torch.from_numpy(X_test)\n",
        "Xt_test= Xt_test.float()\n",
        "yt_test = torch.from_numpy(y_test)\n",
        "yt_test= yt_test.float()\n",
        "Xt_test, yt_test = Xt_test.to(device), yt_test.to(device)"
      ],
      "metadata": {
        "id": "qkaZAH2x5Zej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step=1\n",
        "train_acc_arr=[]\n",
        "test_acc_arr=[]\n",
        "loss_epoch_arr=[]\n",
        "for epoch in range(128):\n",
        "    Xt_train= Xt_train.float()\n",
        "    Xt_train = Xt_train.to(device)\n",
        "    yt_train = yt_train.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    pred = net(Xt_train)\n",
        "    loss = criterion(pred, yt_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_acc = ((torch.sum(pred == yt_train)) / len(Xt_train)) * 100\n",
        "    train_acc_arr.append(train_acc)\n",
        "    test_pred = net(Xt_test)\n",
        "    test_acc = ((torch.sum(test_pred == yt_test)) / len(Xt_test)) * 100\n",
        "    test_acc_arr.append(test_acc)\n",
        "    loss_epoch_arr.append(loss.item())\n",
        "    print(f'Step {step} ==> loss {loss:.4f} ==> train-accuracy {train_acc:.2f} ==> test-accuracy {test_acc:.2f}')\n",
        "    step=step+1"
      ],
      "metadata": {
        "id": "mzdIERzR5bK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f6acce-27fd-40e4-e8c6-9f9cd77548c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 ==> loss 0.3731 ==> train-accuracy 99.50 ==> test-accuracy 83.33\n",
            "Step 2 ==> loss 0.3690 ==> train-accuracy 99.33 ==> test-accuracy 80.67\n",
            "Step 3 ==> loss 0.3613 ==> train-accuracy 96.14 ==> test-accuracy 76.00\n",
            "Step 4 ==> loss 0.3786 ==> train-accuracy 85.40 ==> test-accuracy 82.67\n",
            "Step 5 ==> loss 0.3625 ==> train-accuracy 98.83 ==> test-accuracy 83.33\n",
            "Step 6 ==> loss 0.3692 ==> train-accuracy 99.50 ==> test-accuracy 83.33\n",
            "Step 7 ==> loss 0.3674 ==> train-accuracy 99.50 ==> test-accuracy 80.67\n",
            "Step 8 ==> loss 0.3602 ==> train-accuracy 97.65 ==> test-accuracy 77.33\n",
            "Step 9 ==> loss 0.3751 ==> train-accuracy 86.24 ==> test-accuracy 80.67\n",
            "Step 10 ==> loss 0.3600 ==> train-accuracy 98.15 ==> test-accuracy 83.33\n",
            "Step 11 ==> loss 0.3650 ==> train-accuracy 99.50 ==> test-accuracy 83.33\n",
            "Step 12 ==> loss 0.3641 ==> train-accuracy 99.50 ==> test-accuracy 80.67\n",
            "Step 13 ==> loss 0.3588 ==> train-accuracy 98.15 ==> test-accuracy 77.33\n",
            "Step 14 ==> loss 0.3723 ==> train-accuracy 85.74 ==> test-accuracy 81.33\n",
            "Step 15 ==> loss 0.3599 ==> train-accuracy 98.66 ==> test-accuracy 83.33\n",
            "Step 16 ==> loss 0.3658 ==> train-accuracy 99.50 ==> test-accuracy 83.33\n",
            "Step 17 ==> loss 0.3646 ==> train-accuracy 99.50 ==> test-accuracy 81.33\n",
            "Step 18 ==> loss 0.3584 ==> train-accuracy 98.66 ==> test-accuracy 77.33\n",
            "Step 19 ==> loss 0.3703 ==> train-accuracy 85.91 ==> test-accuracy 81.33\n",
            "Step 20 ==> loss 0.3580 ==> train-accuracy 98.49 ==> test-accuracy 83.33\n",
            "Step 21 ==> loss 0.3625 ==> train-accuracy 99.50 ==> test-accuracy 82.67\n",
            "Step 22 ==> loss 0.3618 ==> train-accuracy 99.33 ==> test-accuracy 80.67\n",
            "Step 23 ==> loss 0.3571 ==> train-accuracy 98.15 ==> test-accuracy 77.33\n",
            "Step 24 ==> loss 0.3654 ==> train-accuracy 88.09 ==> test-accuracy 80.67\n",
            "Step 25 ==> loss 0.3567 ==> train-accuracy 98.15 ==> test-accuracy 82.67\n",
            "Step 26 ==> loss 0.3597 ==> train-accuracy 99.33 ==> test-accuracy 82.67\n",
            "Step 27 ==> loss 0.3588 ==> train-accuracy 99.16 ==> test-accuracy 80.67\n",
            "Step 28 ==> loss 0.3558 ==> train-accuracy 96.48 ==> test-accuracy 77.33\n",
            "Step 29 ==> loss 0.3601 ==> train-accuracy 90.44 ==> test-accuracy 80.67\n",
            "Step 30 ==> loss 0.3563 ==> train-accuracy 98.83 ==> test-accuracy 82.00\n",
            "Step 31 ==> loss 0.3582 ==> train-accuracy 99.50 ==> test-accuracy 80.67\n",
            "Step 32 ==> loss 0.3562 ==> train-accuracy 98.83 ==> test-accuracy 78.00\n",
            "Step 33 ==> loss 0.3567 ==> train-accuracy 92.45 ==> test-accuracy 78.67\n",
            "Step 34 ==> loss 0.3546 ==> train-accuracy 96.31 ==> test-accuracy 80.00\n",
            "Step 35 ==> loss 0.3555 ==> train-accuracy 98.99 ==> test-accuracy 80.00\n",
            "Step 36 ==> loss 0.3549 ==> train-accuracy 98.99 ==> test-accuracy 78.67\n",
            "Step 37 ==> loss 0.3550 ==> train-accuracy 93.62 ==> test-accuracy 79.33\n",
            "Step 38 ==> loss 0.3538 ==> train-accuracy 97.15 ==> test-accuracy 80.00\n",
            "Step 39 ==> loss 0.3541 ==> train-accuracy 98.66 ==> test-accuracy 79.33\n",
            "Step 40 ==> loss 0.3534 ==> train-accuracy 97.15 ==> test-accuracy 78.67\n",
            "Step 41 ==> loss 0.3534 ==> train-accuracy 95.64 ==> test-accuracy 79.33\n",
            "Step 42 ==> loss 0.3530 ==> train-accuracy 97.82 ==> test-accuracy 79.33\n",
            "Step 43 ==> loss 0.3529 ==> train-accuracy 97.82 ==> test-accuracy 78.67\n",
            "Step 44 ==> loss 0.3527 ==> train-accuracy 95.81 ==> test-accuracy 79.33\n",
            "Step 45 ==> loss 0.3525 ==> train-accuracy 98.32 ==> test-accuracy 79.33\n",
            "Step 46 ==> loss 0.3522 ==> train-accuracy 96.64 ==> test-accuracy 79.33\n",
            "Step 47 ==> loss 0.3520 ==> train-accuracy 96.98 ==> test-accuracy 79.33\n",
            "Step 48 ==> loss 0.3518 ==> train-accuracy 97.82 ==> test-accuracy 78.67\n",
            "Step 49 ==> loss 0.3518 ==> train-accuracy 95.64 ==> test-accuracy 79.33\n",
            "Step 50 ==> loss 0.3521 ==> train-accuracy 99.16 ==> test-accuracy 79.33\n",
            "Step 51 ==> loss 0.3513 ==> train-accuracy 97.65 ==> test-accuracy 78.67\n",
            "Step 52 ==> loss 0.3531 ==> train-accuracy 91.11 ==> test-accuracy 80.00\n",
            "Step 53 ==> loss 0.3553 ==> train-accuracy 99.83 ==> test-accuracy 82.67\n",
            "Step 54 ==> loss 0.3573 ==> train-accuracy 100.00 ==> test-accuracy 80.00\n",
            "Step 55 ==> loss 0.3536 ==> train-accuracy 99.66 ==> test-accuracy 78.67\n",
            "Step 56 ==> loss 0.3552 ==> train-accuracy 89.60 ==> test-accuracy 79.33\n",
            "Step 57 ==> loss 0.3518 ==> train-accuracy 98.99 ==> test-accuracy 80.00\n",
            "Step 58 ==> loss 0.3531 ==> train-accuracy 99.66 ==> test-accuracy 79.33\n",
            "Step 59 ==> loss 0.3515 ==> train-accuracy 98.99 ==> test-accuracy 78.67\n",
            "Step 60 ==> loss 0.3529 ==> train-accuracy 91.61 ==> test-accuracy 79.33\n",
            "Step 61 ==> loss 0.3511 ==> train-accuracy 99.16 ==> test-accuracy 79.33\n",
            "Step 62 ==> loss 0.3518 ==> train-accuracy 99.66 ==> test-accuracy 79.33\n",
            "Step 63 ==> loss 0.3501 ==> train-accuracy 97.99 ==> test-accuracy 78.00\n",
            "Step 64 ==> loss 0.3529 ==> train-accuracy 90.77 ==> test-accuracy 80.67\n",
            "Step 65 ==> loss 0.3528 ==> train-accuracy 99.83 ==> test-accuracy 81.33\n",
            "Step 66 ==> loss 0.3551 ==> train-accuracy 100.00 ==> test-accuracy 80.67\n",
            "Step 67 ==> loss 0.3524 ==> train-accuracy 99.83 ==> test-accuracy 78.67\n",
            "Step 68 ==> loss 0.3503 ==> train-accuracy 92.79 ==> test-accuracy 79.33\n",
            "Step 69 ==> loss 0.3493 ==> train-accuracy 96.48 ==> test-accuracy 79.33\n",
            "Step 70 ==> loss 0.3499 ==> train-accuracy 99.50 ==> test-accuracy 79.33\n",
            "Step 71 ==> loss 0.3493 ==> train-accuracy 99.50 ==> test-accuracy 78.00\n",
            "Step 72 ==> loss 0.3506 ==> train-accuracy 90.77 ==> test-accuracy 80.67\n",
            "Step 73 ==> loss 0.3518 ==> train-accuracy 99.83 ==> test-accuracy 80.67\n",
            "Step 74 ==> loss 0.3540 ==> train-accuracy 100.00 ==> test-accuracy 80.67\n",
            "Step 75 ==> loss 0.3515 ==> train-accuracy 99.83 ==> test-accuracy 78.67\n",
            "Step 76 ==> loss 0.3486 ==> train-accuracy 95.13 ==> test-accuracy 78.00\n",
            "Step 77 ==> loss 0.3517 ==> train-accuracy 89.09 ==> test-accuracy 81.33\n",
            "Step 78 ==> loss 0.3545 ==> train-accuracy 100.00 ==> test-accuracy 84.00\n",
            "Step 79 ==> loss 0.3610 ==> train-accuracy 100.00 ==> test-accuracy 83.33\n",
            "Step 80 ==> loss 0.3589 ==> train-accuracy 100.00 ==> test-accuracy 80.67\n",
            "Step 81 ==> loss 0.3511 ==> train-accuracy 100.00 ==> test-accuracy 77.33\n",
            "Step 82 ==> loss 0.3585 ==> train-accuracy 85.40 ==> test-accuracy 80.67\n",
            "Step 83 ==> loss 0.3501 ==> train-accuracy 99.66 ==> test-accuracy 82.67\n",
            "Step 84 ==> loss 0.3548 ==> train-accuracy 100.00 ==> test-accuracy 82.67\n",
            "Step 85 ==> loss 0.3546 ==> train-accuracy 100.00 ==> test-accuracy 80.67\n",
            "Step 86 ==> loss 0.3500 ==> train-accuracy 99.66 ==> test-accuracy 78.00\n",
            "Step 87 ==> loss 0.3512 ==> train-accuracy 91.78 ==> test-accuracy 78.00\n",
            "Step 88 ==> loss 0.3485 ==> train-accuracy 96.14 ==> test-accuracy 80.00\n",
            "Step 89 ==> loss 0.3489 ==> train-accuracy 99.66 ==> test-accuracy 80.67\n",
            "Step 90 ==> loss 0.3495 ==> train-accuracy 99.83 ==> test-accuracy 79.33\n",
            "Step 91 ==> loss 0.3480 ==> train-accuracy 99.16 ==> test-accuracy 78.00\n",
            "Step 92 ==> loss 0.3485 ==> train-accuracy 94.30 ==> test-accuracy 78.00\n",
            "Step 93 ==> loss 0.3471 ==> train-accuracy 97.65 ==> test-accuracy 79.33\n",
            "Step 94 ==> loss 0.3475 ==> train-accuracy 99.16 ==> test-accuracy 79.33\n",
            "Step 95 ==> loss 0.3473 ==> train-accuracy 99.16 ==> test-accuracy 78.67\n",
            "Step 96 ==> loss 0.3468 ==> train-accuracy 97.32 ==> test-accuracy 78.67\n",
            "Step 97 ==> loss 0.3471 ==> train-accuracy 95.97 ==> test-accuracy 79.33\n",
            "Step 98 ==> loss 0.3469 ==> train-accuracy 99.50 ==> test-accuracy 79.33\n",
            "Step 99 ==> loss 0.3468 ==> train-accuracy 99.50 ==> test-accuracy 78.67\n",
            "Step 100 ==> loss 0.3461 ==> train-accuracy 97.65 ==> test-accuracy 78.00\n",
            "Step 101 ==> loss 0.3470 ==> train-accuracy 92.95 ==> test-accuracy 79.33\n",
            "Step 102 ==> loss 0.3487 ==> train-accuracy 100.00 ==> test-accuracy 80.67\n",
            "Step 103 ==> loss 0.3506 ==> train-accuracy 100.00 ==> test-accuracy 80.00\n",
            "Step 104 ==> loss 0.3490 ==> train-accuracy 100.00 ==> test-accuracy 78.67\n",
            "Step 105 ==> loss 0.3458 ==> train-accuracy 98.32 ==> test-accuracy 77.33\n",
            "Step 106 ==> loss 0.3565 ==> train-accuracy 86.24 ==> test-accuracy 81.33\n",
            "Step 107 ==> loss 0.3521 ==> train-accuracy 100.00 ==> test-accuracy 85.33\n",
            "Step 108 ==> loss 0.3627 ==> train-accuracy 100.00 ==> test-accuracy 85.33\n",
            "Step 109 ==> loss 0.3640 ==> train-accuracy 100.00 ==> test-accuracy 83.33\n",
            "Step 110 ==> loss 0.3552 ==> train-accuracy 100.00 ==> test-accuracy 80.00\n",
            "Step 111 ==> loss 0.3462 ==> train-accuracy 99.16 ==> test-accuracy 76.00\n",
            "Step 112 ==> loss 0.3766 ==> train-accuracy 82.72 ==> test-accuracy 80.67\n",
            "Step 113 ==> loss 0.3486 ==> train-accuracy 99.83 ==> test-accuracy 84.67\n",
            "Step 114 ==> loss 0.3600 ==> train-accuracy 100.00 ==> test-accuracy 86.00\n",
            "Step 115 ==> loss 0.3658 ==> train-accuracy 100.00 ==> test-accuracy 84.67\n",
            "Step 116 ==> loss 0.3597 ==> train-accuracy 100.00 ==> test-accuracy 80.67\n",
            "Step 117 ==> loss 0.3495 ==> train-accuracy 100.00 ==> test-accuracy 78.67\n",
            "Step 118 ==> loss 0.3533 ==> train-accuracy 90.60 ==> test-accuracy 78.67\n",
            "Step 119 ==> loss 0.3514 ==> train-accuracy 91.44 ==> test-accuracy 80.67\n",
            "Step 120 ==> loss 0.3484 ==> train-accuracy 100.00 ==> test-accuracy 84.67\n",
            "Step 121 ==> loss 0.3538 ==> train-accuracy 100.00 ==> test-accuracy 84.67\n",
            "Step 122 ==> loss 0.3548 ==> train-accuracy 100.00 ==> test-accuracy 81.33\n",
            "Step 123 ==> loss 0.3508 ==> train-accuracy 100.00 ==> test-accuracy 80.00\n",
            "Step 124 ==> loss 0.3456 ==> train-accuracy 99.66 ==> test-accuracy 77.33\n",
            "Step 125 ==> loss 0.3573 ==> train-accuracy 85.91 ==> test-accuracy 80.00\n",
            "Step 126 ==> loss 0.3467 ==> train-accuracy 99.83 ==> test-accuracy 83.33\n",
            "Step 127 ==> loss 0.3525 ==> train-accuracy 100.00 ==> test-accuracy 84.67\n",
            "Step 128 ==> loss 0.3548 ==> train-accuracy 100.00 ==> test-accuracy 83.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KsPVcUVY8RZJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}